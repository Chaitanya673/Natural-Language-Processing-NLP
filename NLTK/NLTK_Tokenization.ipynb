{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "! pip install NLTK",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: NLTK in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (3.3)\nRequirement already satisfied: six in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from NLTK) (1.11.0)\n\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.download('punkt')",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text_file = open(\"demo_text.txt\",'r')\nmy_text = text_file.read()\nprint(my_text)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Hi Learner! Welcome to NLP (Natural Language Processing) with Python. Here you will learn text mining and processing on natural language data. Are you aware about the Python basics? \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1. Word Tokenization "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import word_tokenize\n\nword_tokens = word_tokenize(my_text)\nprint(word_tokens) # print function requires Python 3",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi', 'Learner', '!', 'Welcome', 'to', 'NLP', '(', 'Natural', 'Language', 'Processing', ')', 'with', 'Python', '.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', '.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics', '?']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2. Sentences Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize\n\nsent_tokens = sent_tokenize(my_text)\nprint(sent_tokens) # print function requires Python 3",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi Learner!', 'Welcome to NLP (Natural Language Processing) with Python.', 'Here you will learn text mining and processing on natural language data.', 'Are you aware about the Python basics?']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3. Tokenization (N-Grams)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Creating Bigrams"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.util import ngrams\n\nmy_words = word_tokenize(my_text) # This is the list of all words\ntwograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\nprint(twograms)",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[('Hi', 'Learner'), ('Learner', '!'), ('!', 'Welcome'), ('Welcome', 'to'), ('to', 'NLP'), ('NLP', '('), ('(', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing'), ('Processing', ')'), (')', 'with'), ('with', 'Python'), ('Python', '.'), ('.', 'Here'), ('Here', 'you'), ('you', 'will'), ('will', 'learn'), ('learn', 'text'), ('text', 'mining'), ('mining', 'and'), ('and', 'processing'), ('processing', 'on'), ('on', 'natural'), ('natural', 'language'), ('language', 'data'), ('data', '.'), ('.', 'Are'), ('Are', 'you'), ('you', 'aware'), ('aware', 'about'), ('about', 'the'), ('the', 'Python'), ('Python', 'basics'), ('basics', '?')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "bigrams = list(nltk.bigrams(my_words))\nprint(bigrams)",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[('Hi', 'Learner'), ('Learner', '!'), ('!', 'Welcome'), ('Welcome', 'to'), ('to', 'NLP'), ('NLP', '('), ('(', 'Natural'), ('Natural', 'Language'), ('Language', 'Processing'), ('Processing', ')'), (')', 'with'), ('with', 'Python'), ('Python', '.'), ('.', 'Here'), ('Here', 'you'), ('you', 'will'), ('will', 'learn'), ('learn', 'text'), ('text', 'mining'), ('mining', 'and'), ('and', 'processing'), ('processing', 'on'), ('on', 'natural'), ('natural', 'language'), ('language', 'data'), ('data', '.'), ('.', 'Are'), ('Are', 'you'), ('you', 'aware'), ('aware', 'about'), ('about', 'the'), ('the', 'Python'), ('Python', 'basics'), ('basics', '?')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Creating trigrams"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "threegrams = list(ngrams(my_words,3))\nprint(threegrams)",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[('Hi', 'Learner', '!'), ('Learner', '!', 'Welcome'), ('!', 'Welcome', 'to'), ('Welcome', 'to', 'NLP'), ('to', 'NLP', '('), ('NLP', '(', 'Natural'), ('(', 'Natural', 'Language'), ('Natural', 'Language', 'Processing'), ('Language', 'Processing', ')'), ('Processing', ')', 'with'), (')', 'with', 'Python'), ('with', 'Python', '.'), ('Python', '.', 'Here'), ('.', 'Here', 'you'), ('Here', 'you', 'will'), ('you', 'will', 'learn'), ('will', 'learn', 'text'), ('learn', 'text', 'mining'), ('text', 'mining', 'and'), ('mining', 'and', 'processing'), ('and', 'processing', 'on'), ('processing', 'on', 'natural'), ('on', 'natural', 'language'), ('natural', 'language', 'data'), ('language', 'data', '.'), ('data', '.', 'Are'), ('.', 'Are', 'you'), ('Are', 'you', 'aware'), ('you', 'aware', 'about'), ('aware', 'about', 'the'), ('about', 'the', 'Python'), ('the', 'Python', 'basics'), ('Python', 'basics', '?')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "trigrams = list(nltk.trigrams(my_words))\nprint(trigrams)",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[('Hi', 'Learner', '!'), ('Learner', '!', 'Welcome'), ('!', 'Welcome', 'to'), ('Welcome', 'to', 'NLP'), ('to', 'NLP', '('), ('NLP', '(', 'Natural'), ('(', 'Natural', 'Language'), ('Natural', 'Language', 'Processing'), ('Language', 'Processing', ')'), ('Processing', ')', 'with'), (')', 'with', 'Python'), ('with', 'Python', '.'), ('Python', '.', 'Here'), ('.', 'Here', 'you'), ('Here', 'you', 'will'), ('you', 'will', 'learn'), ('will', 'learn', 'text'), ('learn', 'text', 'mining'), ('text', 'mining', 'and'), ('mining', 'and', 'processing'), ('and', 'processing', 'on'), ('processing', 'on', 'natural'), ('on', 'natural', 'language'), ('natural', 'language', 'data'), ('language', 'data', '.'), ('data', '.', 'Are'), ('.', 'Are', 'you'), ('Are', 'you', 'aware'), ('you', 'aware', 'about'), ('aware', 'about', 'the'), ('about', 'the', 'Python'), ('the', 'Python', 'basics'), ('Python', 'basics', '?')]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1. Tokenization using Python’s split() function\nLet’s start with the split() method as it is the most basic one. It returns a list of strings after breaking the given string by the specified separator. By default, split() breaks a string at each space. We can change the separator to anything. Let’s check it out."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Word Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "split_word_tokens = my_text.split()\nprint(split_tokens)",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi', 'Learner!', 'Welcome', 'to', 'NLP', '(Natural', 'Language', 'Processing)', 'with', 'Python.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics?']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Sentence Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "split_sent_tokens = my_text.split('. ')\nprint(split_sent_tokens)",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi Learner! Welcome to NLP (Natural Language Processing) with Python', 'Here you will learn text mining and processing on natural language data', 'Are you aware about the Python basics? ']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2. Tokenization using Regular Expressions (RegEx)\nFirst, let’s understand what a regular expression is. It is basically a special character sequence that helps you match or find other strings or sets of strings using that sequence as a pattern.\n\nWe can use the re library in Python to work with regular expression. This library comes preinstalled with the Python installation package.\n\nNow, let’s perform word tokenization and sentence tokenization keeping RegEx in mind."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Word Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\nfrom nltk.tokenize import RegexpTokenizer\n\n# RegEx Tokenizer with whitespace delimiter\nwhitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps = True)\n\nwhitespace_tokens = whitespace_tokenizer.tokenize(my_text)\nprint(whitespace_tokens)",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi', 'Learner!', 'Welcome', 'to', 'NLP', '(Natural', 'Language', 'Processing)', 'with', 'Python.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics?']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "re_word_tokens = re.findall(\"[\\w']+\", my_text)\nprint(re_word_tokens)",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi', 'Learner', 'Welcome', 'to', 'NLP', 'Natural', 'Language', 'Processing', 'with', 'Python', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# RegexpTokenizer to match only capitalized words\ncap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\nprint(cap_tokenizer.tokenize(my_text))",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi', 'Learner', 'Welcome', 'NLP', 'Natural', 'Language', 'Processing', 'Python', 'Here', 'Are', 'Python']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Sentence Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "re_sentence_tokens = re.compile('[.!?] ').split(my_text)\nprint(re_sentence_tokens)",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi Learner', 'Welcome to NLP (Natural Language Processing) with Python', 'Here you will learn text mining and processing on natural language data', 'Are you aware about the Python basics', '']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3. Tokenization using NLTK\n\nNLTK contains a module called tokenize() which further classifies into two sub-categories:\n\nWord tokenize: We use the word_tokenize() method to split a sentence into tokens or words\nSentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Word Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import word_tokenize\n\nnltk_word_tokens = word_tokenize(my_text)\nprint(nltk_word_tokens)",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi', 'Learner', '!', 'Welcome', 'to', 'NLP', '(', 'Natural', 'Language', 'Processing', ')', 'with', 'Python', '.', 'Here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', '.', 'Are', 'you', 'aware', 'about', 'the', 'Python', 'basics', '?']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Sentence Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize\n\nnltk_sent_tokens = sent_tokenize(my_text)\nprint(nltk_sent_tokens)",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Hi Learner!', 'Welcome to NLP (Natural Language Processing) with Python.', 'Here you will learn text mining and processing on natural language data.', 'Are you aware about the Python basics?']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4. Tokenization using Keras\nKeras! One of the hottest deep learning frameworks in the industry right now. It is an open-source neural network library for Python. Keras is super easy to use and can also run on top of TensorFlow.\n\nIn the NLP context, we can use Keras for cleaning the unstructured text data that we typically collect."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Word Tokenization"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import text_to_word_sequence",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "keras_word_tokens = text_to_word_sequence(my_text)\nprint(keras_word_tokens)",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['hi', 'learner', 'welcome', 'to', 'nlp', 'natural', 'language', 'processing', 'with', 'python', 'here', 'you', 'will', 'learn', 'text', 'mining', 'and', 'processing', 'on', 'natural', 'language', 'data', 'are', 'you', 'aware', 'about', 'the', 'python', 'basics']\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}